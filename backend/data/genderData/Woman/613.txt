There isn’t any evidence to support President Trump’s assertion that three to five million illegal votes were cast in the 2016 presidential election. But there is one study that has been interpreted to suggest it is at least possible. It found that between 32, 000 and 2. 8 million noncitizen voters might have fraudulently cast ballots in the 2008 presidential election. The study, based on a survey of 38, 000 people after that election, has been under fire since it was published in 2014. Now even its authors concede that it probably overstated the amount of noncitizen voting. “The   estimates are likely incorrect,” Jesse Richman, one of the   of the study and a political science professor at Old Dominion University, said in an email exchange on Wednesday. In a post online, he also said that the findings do not support Mr. Trump’s contention that millions cast ballots illegally. Mr. Richman still maintains that some small percentage of noncitizens vote in American elections. But the debate over this study has moved on. It’s no longer about whether millions of illegal votes were cast, but whether there’s any evidence for noncitizen voting at all. The study’s bold claims fell apart because of something called response error: the possibility that people taking a survey don’t answer a question correctly  —   in this case, a question about being American citizens. There is always a tiny amount of response error in surveys. Respondents might not understand the question. Or they might understand it, but mark the wrong answer by mistake, if the survey is  . An interviewer, if there is one, could accidentally record the wrong answer. Such errors usually aren’t a problem large enough to change the results of a survey. But both the survey and the question posed by researchers were unusual. The survey  —   the Cooperative Congressional Election Study  —   was huge, with 38, 000 respondents in 2008. And the group in question  —   noncitizens  —   was very small, just 339 of those respondents. The problem is that even a tiny amount of response error among the 38, 000 respondents could have been enough to contaminate the results of the tiny group of noncitizens. Imagine, for instance, that 99. 9 percent of people respond to the survey’s citizenship question correctly. In such a big survey, even that high success rate would still imply that there were 38 respondents who answered incorrectly  —   enough to make up a big chunk of the tiny pool of 339 noncitizen respondents. If those 38 misreported noncitizens had indeed voted, then suddenly it would look as if 10 percent of noncitizens voted. This critique could explain all of the noncitizen voting observed in the study. Critics of the study  —   Stephen Ansolabehere, a Harvard political scientist, Samantha Luks, a statistician at YouGov, and Brian  Schaffner, a political scientist at the University of Massachusetts Amherst  —   were able to marshal evidence strongly consistent with that possibility, because of the survey’s unusual design: Thousands of voters are   in subsequent elections. That allowed the study’s critics to check whether people were consistent about their answer on the citizenship answer from year to year. If the people were consistent, they were probably noncitizens. If voters were inconsistent, it would be a sign that the category was contaminated by the tiny number of voters who misreported their citizenship. There was not much consistency. Between 2010 and 2012, 20 voters switched from being citizens to noncitizens (an extremely unlikely transition) and 36 others switched from noncitizens to citizens (a more common transition, but one reported at a far greater rate than typically occurs). These shifting answers strongly bolster the theory that many of the respondents logged as noncitizens had responded in error. But most important, among the 85 respondents who said they were noncitizens in both 2010 and 2012  —   those most likely to really be noncitizens  —   none had voted in the 2010 midterm elections. The critics concluded that “the likely percent of noncitizen voters in recent U. S. elections is 0. ” In a response published in October, Mr. Richman and his colleagues did not contest the finding that measurement error probably exaggerated the number of noncitizen voters. “The response error issues they focus on may have biased our numbers,” Mr. Richman said in an email to The New York Times on Wednesday. Mr. Richman and his colleagues have not estimated a new range of possible noncitizen voting. Instead, the October response sought only to rebut the notion that there was no noncitizen voting. They argued that measurement error couldn’t explain all of the people who said they were noncitizens and voted. When it came to hard evidence immune to the measurement error critique, Mr. Richman and his   found one validated 2012 voter who had indicated not being a citizen in both the 2010 and 2012 surveys. But the same noncitizen had indicated in the survey that he or she was not registered to vote. The determination that he or she was a voter was based on voter records: The respondents to the survey were matched to a voter registration file. It is possible that this noncitizen was erroneously matched to the voter file. The matching process is good but imperfect, and becomes harder with less information  —   like the absence of a specific address or date of birth. That the respondent said he or she wasn’t registered certainly raises the possibility that the match was wrong. “I haven’t seen any evidence that I would say shows that any noncitizens vote,” Mr. Schaffner said. “That doesn’t mean that the rate is exactly zero. But it does mean that it’s   frequency that we can’t even measure it with traditional methods. ”